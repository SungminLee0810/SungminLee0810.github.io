---
title: "DETR: End-to-End Object Detection with Transformers"
date: 2020-06-15 00:00:00
categories: Paper_review Object_detection ECCV2020
---

# Introduction:

우선 DETR은 제목에서도 확인할 수 있는 것처럼 End-to-End로 학습할 수 있는 Object Detection 방법에 대한 논문이다.
DETR은 Facebook AI에서 발표한 논문으로 아직 ECCV 2020의 accepted paper list가 공개되지 않았음에도,
ML 및 CV 커뮤니티로부터 뜨거운 주목을 받고 있다.

이 논문이 주목을 받는 것은 기존 방법들과 차별된 두 가지 이유가 있기 때문이라 생각한다. 
- Anchor를 사용하지 않는다.
- Non-maximum Suppression (NMS)를 사용하지 않는다.

위 두 모듈은 기존 Object Detection 방법들에서는 필수적으로 사용되던 방법이지만, 생각해보면 heuristic의 집약체라 해도 과언이 아니다. 
본 논문에서는 이러한 문제점을 지적하며 Transformer와 Hungarian algorithm을 적용하여,
Anchor와 NMS 조차도 모델이 알아서 해결할 수 있음을 실험적으로 보여주었다.
(시퀀셜 데이터 분석에 유리하다고 알려진 Transformer를 Image 데이터에 적용한 것도 이 논문이 주목을 받은 이유 중 하나가 될 수 있겠다.)

![image-center]({{ site.url }}{{ site.baseurl }}/assets/images/2020_0615/fig1_frcnn_vs_detr.png){: .align-center}
*Fig.1 Faster RCNN과 DETR의 구조적 차이 비교 (source: https://medium.com/swlh/one-stop-for-object-detectors-2c99daa08c50)*

Fig.1은 Faster RCNN과 DETR의 구조적 차이를 도식화하여 표현한 것이다. 
Faster RCNN이 object를 detection 하기 위해서는 CNN feature를 추출한 뒤 region proposal, NMS, 그리고 RoI Align과 같은 domain specific한 복잡한 과정을 거치는 반면,
DETR은 이 모든 과정을 Transformer에 맡겨버린다. 
저자들은 Detection 모델이 학습을 잘하였다면 기존 detection 방법들에 적용하던 heuristic한 과정들을 추론 과정에서 모두 녹여낼 것이라는 가정을 하였을 것이다.
(무책임해 보일 수 있지만 실험 결과를 보면 의도한대로 잘 학습이 되는 것 같다.)

모델의 구조가 간단하니 DETR은 학습 과정도 End-to-End로 간단하게 구현이 가능하다는 것도 장점이라 할 수 있다.
Faster R-CNN을 학습시키기 위해서는 region proposal network과 detector 몇번의 epoch 마다 번갈아가면서 학습을 시켜줘야한다. (저자들의 original 코드 참조)

반면에 필자의 생각에 DETR에도 몇 가지 문제점이 있다.
첫째는 학습 시간이다. 전체적인 파이프라인은 간단하지만, Transformer 자체는 복잡한 모델이다. (Transformer에 대한 자세한 설명은 다음 [포스팅](/_posts/2020-06-19-tech-post.md)에서 다룰 예정이다.)
Self-attention을 포함한 encoder-decoder 구조를 하고 있어 학습해야하는 파라미터가 많아, V100을 16장을 쓰고도 학습시간이 3일이나 걸린다고 한다.
둘째는 small object에 대한 detection 성능이다.

# Method:
